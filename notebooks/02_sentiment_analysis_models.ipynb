{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Models - TikNep Dataset\n",
    "\n",
    "This notebook implements and evaluates various models for **Sentiment Analysis (SEN)** task.\n",
    "\n",
    "**Task Type:** Multi-class Classification  \n",
    "**Classes:** 0 = Neutral, 1 = Negative, 2 = Positive  \n",
    "**Dataset:** 3,947 Nepali comments\n",
    "\n",
    "## Models Implemented:\n",
    "1. Machine Learning: SVM, MNB, RF\n",
    "2. Deep Learning: Bi-LSTM, Bi-GRU\n",
    "3. Transformer: BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports for Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"Data libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Load Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-split data\n",
    "X_train = pd.read_csv('../data/splits/sentiment/X_train_sentiment.csv')\n",
    "y_train = pd.read_csv('../data/splits/sentiment/y_train_sentiment.csv')\n",
    "X_val = pd.read_csv('../data/splits/sentiment/X_val_sentiment.csv')\n",
    "y_val = pd.read_csv('../data/splits/sentiment/y_val_sentiment.csv')\n",
    "X_test = pd.read_csv('../data/splits/sentiment/X_test_sentiment.csv')\n",
    "y_test = pd.read_csv('../data/splits/sentiment/y_test_sentiment.csv')\n",
    "\n",
    "print(\"Data loaded successfully!\")\n",
    "print(f\"\\nTrain: {len(X_train)} samples\")\n",
    "print(f\"Validation: {len(X_val)} samples\")\n",
    "print(f\"Test: {len(X_test)} samples\")\n",
    "print(f\"Total: {len(X_train) + len(X_val) + len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 How Data Looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Data Sample:\")\n",
    "print(X_train.head())\n",
    "print(\"\\nTraining Labels Sample:\")\n",
    "print(y_train.head())\n",
    "\n",
    "# Display some example texts\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Sample Comments from Training Set:\")\n",
    "print(\"=\"*80)\n",
    "for idx in range(5):\n",
    "    print(f\"\\n{idx+1}. Text: {X_train.iloc[idx]['Text']}\")\n",
    "    print(f\"   Label: {y_train.iloc[idx].values[0]} (Sentiment)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Summary of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class distribution\n",
    "sentiment_map = {0: 'Neutral', 1: 'Negative', 2: 'Positive'}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, y_data in [('Training', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    print(f\"\\n{dataset_name} Set:\")\n",
    "    counts = y_data.iloc[:, 0].value_counts().sort_index()\n",
    "    for label, count in counts.items():\n",
    "        percentage = (count / len(y_data)) * 100\n",
    "        print(f\"  {label} ({sentiment_map[label]:>8}): {count:4d} samples ({percentage:5.2f}%)\")\n",
    "\n",
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "sentiment_labels = ['Neutral', 'Negative', 'Positive']\n",
    "colors = ['#5DADE2', '#E74C3C', '#52BE80']\n",
    "\n",
    "for idx, (y_data, title) in enumerate([(y_train, 'Training'), (y_val, 'Validation'), (y_test, 'Test')]):\n",
    "    counts = y_data.iloc[:, 0].value_counts().sort_index()\n",
    "    axes[idx].bar(sentiment_labels, counts.values, color=colors)\n",
    "    axes[idx].set_title(f'{title} Set - Class Distribution', fontweight='bold', fontsize=12)\n",
    "    axes[idx].set_ylabel('Count', fontsize=11)\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "    for i, v in enumerate(counts.values):\n",
    "        axes[idx].text(i, v + 10, str(v), ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Machine Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Imports for SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix\n",
    ")\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"SVM libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Build SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF Vectorization with word n-grams (1-2)\n",
    "print(\"Creating TF-IDF features...\")\n",
    "print(\"Configuration: Word n-grams (1-2), max_features=50,000\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=50000,      # 50K features as per requirement\n",
    "    ngram_range=(1, 2),      # Word unigrams and bigrams\n",
    "    sublinear_tf=True        # Apply sublinear tf scaling (1 + log(tf))\n",
    ")\n",
    "\n",
    "# Fit on training data only, then transform all sets\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train.iloc[:, 0])\n",
    "X_val_tfidf = tfidf_vectorizer.transform(X_val.iloc[:, 0])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test.iloc[:, 0])\n",
    "\n",
    "print(f\"TF-IDF vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")\n",
    "print(f\"X_train_tfidf shape: {X_train_tfidf.shape}\")\n",
    "print(f\"X_val_tfidf shape: {X_val_tfidf.shape}\")\n",
    "print(f\"X_test_tfidf shape: {X_test_tfidf.shape}\")\n",
    "\n",
    "# Train LinearSVC with balanced class weights\n",
    "print(\"\\nTraining LinearSVC model...\")\n",
    "svm_model = LinearSVC(\n",
    "    class_weight='balanced',  # Handle class imbalance\n",
    "    random_state=RANDOM_SEED,\n",
    "    max_iter=2000,            # Increase max iterations for convergence\n",
    "    dual=False                # Use primal formulation (faster for large n_features)\n",
    ")\n",
    "\n",
    "svm_model.fit(X_train_tfidf, y_train.iloc[:, 0])\n",
    "print(\"SVM training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Inference on SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_svm = svm_model.predict(X_train_tfidf)\n",
    "y_val_pred_svm = svm_model.predict(X_val_tfidf)\n",
    "y_test_pred_svm = svm_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SVM - EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Validation Set Metrics\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "print(\"-\"*100)\n",
    "val_accuracy = accuracy_score(y_val.iloc[:, 0], y_val_pred_svm)\n",
    "val_macro_precision = precision_score(y_val.iloc[:, 0], y_val_pred_svm, average='macro')\n",
    "val_macro_recall = recall_score(y_val.iloc[:, 0], y_val_pred_svm, average='macro')\n",
    "val_macro_f1 = f1_score(y_val.iloc[:, 0], y_val_pred_svm, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {val_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {val_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {val_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {val_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val.iloc[:, 0], y_val_pred_svm, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Test Set Metrics\n",
    "print(\"\\nTEST SET:\")\n",
    "print(\"-\"*100)\n",
    "test_accuracy = accuracy_score(y_test.iloc[:, 0], y_test_pred_svm)\n",
    "test_macro_precision = precision_score(y_test.iloc[:, 0], y_test_pred_svm, average='macro')\n",
    "test_macro_recall = recall_score(y_test.iloc[:, 0], y_test_pred_svm, average='macro')\n",
    "test_macro_f1 = f1_score(y_test.iloc[:, 0], y_test_pred_svm, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {test_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {test_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {test_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.iloc[:, 0], y_test_pred_svm, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_test = confusion_matrix(y_test.iloc[:, 0], y_test_pred_svm)\n",
    "print(cm_test)\n",
    "\n",
    "# Visualize Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Confusion Matrix\n",
    "cm_val = confusion_matrix(y_val.iloc[:, 0], y_val_pred_svm)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues', ax=axes[0],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[0].set_title('SVM - Confusion Matrix (Validation Set)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Test Confusion Matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues', ax=axes[1],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[1].set_title('SVM - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Validation - Accuracy: {val_accuracy:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "print(f\"Test       - Accuracy: {test_accuracy:.4f}, Macro F1: {test_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.4 Save SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models/sentiment', exist_ok=True)\n",
    "\n",
    "# Save SVM model\n",
    "with open('../models/sentiment/svm_model.pkl', 'wb') as f:\n",
    "    pickle.dump(svm_model, f)\n",
    "\n",
    "# Save TF-IDF vectorizer\n",
    "with open('../models/sentiment/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_vectorizer, f)\n",
    "\n",
    "print(\"SVM model and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Multinomial Naive Bayes (MNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 Imports for MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "print(\"MNB libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 Build MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Multinomial Naive Bayes\n",
    "print(\"Training Multinomial Naive Bayes model...\")\n",
    "print(\"Configuration: alpha=1.0 (Laplace smoothing)\")\n",
    "\n",
    "mnb_model = MultinomialNB(\n",
    "    alpha=1.0  # Laplace smoothing for handling unseen features\n",
    ")\n",
    "\n",
    "# Note: Using the same TF-IDF features created for SVM\n",
    "# (Word n-grams 1-2, max_features=50,000)\n",
    "mnb_model.fit(X_train_tfidf, y_train.iloc[:, 0])\n",
    "print(\"MNB training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 Inference on MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_mnb = mnb_model.predict(X_train_tfidf)\n",
    "y_val_pred_mnb = mnb_model.predict(X_val_tfidf)\n",
    "y_test_pred_mnb = mnb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"MULTINOMIAL NAIVE BAYES - EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Validation Set Metrics\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "print(\"-\"*100)\n",
    "val_accuracy = accuracy_score(y_val.iloc[:, 0], y_val_pred_mnb)\n",
    "val_macro_precision = precision_score(y_val.iloc[:, 0], y_val_pred_mnb, average='macro')\n",
    "val_macro_recall = recall_score(y_val.iloc[:, 0], y_val_pred_mnb, average='macro')\n",
    "val_macro_f1 = f1_score(y_val.iloc[:, 0], y_val_pred_mnb, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {val_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {val_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {val_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {val_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val.iloc[:, 0], y_val_pred_mnb, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Test Set Metrics\n",
    "print(\"\\nTEST SET:\")\n",
    "print(\"-\"*100)\n",
    "test_accuracy = accuracy_score(y_test.iloc[:, 0], y_test_pred_mnb)\n",
    "test_macro_precision = precision_score(y_test.iloc[:, 0], y_test_pred_mnb, average='macro')\n",
    "test_macro_recall = recall_score(y_test.iloc[:, 0], y_test_pred_mnb, average='macro')\n",
    "test_macro_f1 = f1_score(y_test.iloc[:, 0], y_test_pred_mnb, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {test_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {test_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {test_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.iloc[:, 0], y_test_pred_mnb, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_test = confusion_matrix(y_test.iloc[:, 0], y_test_pred_mnb)\n",
    "print(cm_test)\n",
    "\n",
    "# Visualize Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Confusion Matrix\n",
    "cm_val = confusion_matrix(y_val.iloc[:, 0], y_val_pred_mnb)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Greens', ax=axes[0],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[0].set_title('MNB - Confusion Matrix (Validation Set)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Test Confusion Matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Greens', ax=axes[1],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[1].set_title('MNB - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Validation - Accuracy: {val_accuracy:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "print(f\"Test       - Accuracy: {test_accuracy:.4f}, Macro F1: {test_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.4 Save MNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save MNB model\n",
    "with open('../models/sentiment/mnb_model.pkl', 'wb') as f:\n",
    "    pickle.dump(mnb_model, f)\n",
    "\n",
    "print(\"MNB model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Random Forest (RF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Imports for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "print(\"RF libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Build RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality Reduction with TruncatedSVD\n",
    "# Random Forest performs poorly on sparse high-dimensional data, so we reduce dimensions\n",
    "print(\"Applying dimensionality reduction with TruncatedSVD...\")\n",
    "print(\"Configuration: Reducing to 300 components\")\n",
    "\n",
    "svd = TruncatedSVD(\n",
    "    n_components=300,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "# Fit SVD on training TF-IDF features only, then transform all sets\n",
    "X_train_svd = svd.fit_transform(X_train_tfidf)\n",
    "X_val_svd = svd.transform(X_val_tfidf)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "print(f\"Original TF-IDF shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Reduced SVD shape: {X_train_svd.shape}\")\n",
    "print(f\"Explained variance ratio: {svd.explained_variance_ratio_.sum():.4f}\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "print(\"Configuration: n_estimators=300, max_depth=None, n_jobs=-1\")\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,         # Number of trees\n",
    "    max_depth=None,           # No limit on tree depth\n",
    "    random_state=RANDOM_SEED,\n",
    "    n_jobs=-1,                # Use all available cores\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Note: Using SVD-reduced features instead of raw TF-IDF\n",
    "rf_model.fit(X_train_svd, y_train.iloc[:, 0])\n",
    "print(\"RF training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Inference on RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_rf = rf_model.predict(X_train_svd)\n",
    "y_val_pred_rf = rf_model.predict(X_val_svd)\n",
    "y_test_pred_rf = rf_model.predict(X_test_svd)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"RANDOM FOREST - EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Validation Set Metrics\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "print(\"-\"*100)\n",
    "val_accuracy = accuracy_score(y_val.iloc[:, 0], y_val_pred_rf)\n",
    "val_macro_precision = precision_score(y_val.iloc[:, 0], y_val_pred_rf, average='macro')\n",
    "val_macro_recall = recall_score(y_val.iloc[:, 0], y_val_pred_rf, average='macro')\n",
    "val_macro_f1 = f1_score(y_val.iloc[:, 0], y_val_pred_rf, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {val_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {val_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {val_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {val_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val.iloc[:, 0], y_val_pred_rf, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Test Set Metrics\n",
    "print(\"\\nTEST SET:\")\n",
    "print(\"-\"*100)\n",
    "test_accuracy = accuracy_score(y_test.iloc[:, 0], y_test_pred_rf)\n",
    "test_macro_precision = precision_score(y_test.iloc[:, 0], y_test_pred_rf, average='macro')\n",
    "test_macro_recall = recall_score(y_test.iloc[:, 0], y_test_pred_rf, average='macro')\n",
    "test_macro_f1 = f1_score(y_test.iloc[:, 0], y_test_pred_rf, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {test_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {test_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {test_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.iloc[:, 0], y_test_pred_rf, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_test = confusion_matrix(y_test.iloc[:, 0], y_test_pred_rf)\n",
    "print(cm_test)\n",
    "\n",
    "# Visualize Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Confusion Matrix\n",
    "cm_val = confusion_matrix(y_val.iloc[:, 0], y_val_pred_rf)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Oranges', ax=axes[0],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[0].set_title('RF - Confusion Matrix (Validation Set)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Test Confusion Matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Oranges', ax=axes[1],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[1].set_title('RF - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Feature Importance Analysis\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*100)\n",
    "feature_importance = rf_model.feature_importances_\n",
    "top_k = 10\n",
    "top_indices = np.argsort(feature_importance)[::-1][:top_k]\n",
    "\n",
    "print(f\"\\nTop {top_k} Most Important SVD Components:\")\n",
    "for idx, component_idx in enumerate(top_indices, 1):\n",
    "    print(f\"  {idx}. Component {component_idx}: {feature_importance[component_idx]:.6f}\")\n",
    "\n",
    "# Visualize top feature importances\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(top_k), feature_importance[top_indices], color='coral', edgecolor='black')\n",
    "plt.xlabel('SVD Component Rank', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Importance', fontsize=12, fontweight='bold')\n",
    "plt.title(f'Top {top_k} Most Important SVD Components', fontsize=14, fontweight='bold')\n",
    "plt.xticks(range(top_k), [f'C{i}' for i in top_indices], rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Validation - Accuracy: {val_accuracy:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "print(f\"Test       - Accuracy: {test_accuracy:.4f}, Macro F1: {test_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.4 Save RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save RF model and SVD transformer\n",
    "with open('../models/sentiment/rf_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "with open('../models/sentiment/svd_transformer.pkl', 'wb') as f:\n",
    "    pickle.dump(svd, f)\n",
    "\n",
    "print(\"RF model and SVD transformer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Deep Learning Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Bidirectional LSTM (Bi-LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.1 Imports for Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Set TensorFlow random seed\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "\n",
    "print(\"Bi-LSTM libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.2 Build Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "NUM_WORDS = 50000        # Vocabulary size\n",
    "MAX_LEN = 128            # Maximum sequence length\n",
    "EMBEDDING_DIM = 300      # Embedding dimension\n",
    "LSTM_UNITS = 128         # LSTM units\n",
    "BATCH_SIZE = 32          # Training batch size\n",
    "\n",
    "# Tokenization\n",
    "print(\"Tokenizing text data...\")\n",
    "print(f\"Configuration: num_words={NUM_WORDS}, max_len={MAX_LEN}\")\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token='<OOV>')\n",
    "\n",
    "# Fit tokenizer only on training data\n",
    "tokenizer.fit_on_texts(X_train.iloc[:, 0])\n",
    "\n",
    "# Convert to sequences\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train.iloc[:, 0])\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val.iloc[:, 0])\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test.iloc[:, 0])\n",
    "\n",
    "# Pad sequences to same length\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "\n",
    "print(f\"Vocabulary size: {len(tokenizer.word_index) + 1}\")\n",
    "print(f\"X_train_pad shape: {X_train_pad.shape}\")\n",
    "print(f\"X_val_pad shape: {X_val_pad.shape}\")\n",
    "print(f\"X_test_pad shape: {X_test_pad.shape}\")\n",
    "\n",
    "# Build Bi-LSTM model\n",
    "print(\"\\nBuilding Bi-LSTM model...\")\n",
    "bilstm_model = Sequential([\n",
    "    Embedding(input_dim=NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(LSTM(LSTM_UNITS, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "bilstm_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Bi-LSTM model built successfully!\")\n",
    "bilstm_model.summary()\n",
    "\n",
    "# Train Bi-LSTM\n",
    "print(\"\\nTraining Bi-LSTM model...\")\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "history_bilstm = bilstm_model.fit(\n",
    "    X_train_pad, y_train.iloc[:, 0],\n",
    "    validation_data=(X_val_pad, y_val.iloc[:, 0]),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Bi-LSTM training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.3 Inference on Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_bilstm = np.argmax(bilstm_model.predict(X_train_pad, verbose=0), axis=1)\n",
    "y_val_pred_bilstm = np.argmax(bilstm_model.predict(X_val_pad, verbose=0), axis=1)\n",
    "y_test_pred_bilstm = np.argmax(bilstm_model.predict(X_test_pad, verbose=0), axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BI-LSTM - EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Training Set Metrics\n",
    "print(\"\\nTRAINING SET:\")\n",
    "print(\"-\"*100)\n",
    "train_accuracy = accuracy_score(y_train.iloc[:, 0], y_train_pred_bilstm)\n",
    "train_macro_precision = precision_score(y_train.iloc[:, 0], y_train_pred_bilstm, average='macro')\n",
    "train_macro_recall = recall_score(y_train.iloc[:, 0], y_train_pred_bilstm, average='macro')\n",
    "train_macro_f1 = f1_score(y_train.iloc[:, 0], y_train_pred_bilstm, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {train_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {train_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {train_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {train_macro_f1:.4f}\")\n",
    "\n",
    "# Validation Set Metrics\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "print(\"-\"*100)\n",
    "val_accuracy = accuracy_score(y_val.iloc[:, 0], y_val_pred_bilstm)\n",
    "val_macro_precision = precision_score(y_val.iloc[:, 0], y_val_pred_bilstm, average='macro')\n",
    "val_macro_recall = recall_score(y_val.iloc[:, 0], y_val_pred_bilstm, average='macro')\n",
    "val_macro_f1 = f1_score(y_val.iloc[:, 0], y_val_pred_bilstm, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {val_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {val_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {val_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {val_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val.iloc[:, 0], y_val_pred_bilstm, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Test Set Metrics\n",
    "print(\"\\nTEST SET:\")\n",
    "print(\"-\"*100)\n",
    "test_accuracy = accuracy_score(y_test.iloc[:, 0], y_test_pred_bilstm)\n",
    "test_macro_precision = precision_score(y_test.iloc[:, 0], y_test_pred_bilstm, average='macro')\n",
    "test_macro_recall = recall_score(y_test.iloc[:, 0], y_test_pred_bilstm, average='macro')\n",
    "test_macro_f1 = f1_score(y_test.iloc[:, 0], y_test_pred_bilstm, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {test_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {test_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {test_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.iloc[:, 0], y_test_pred_bilstm, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrices\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_test = confusion_matrix(y_test.iloc[:, 0], y_test_pred_bilstm)\n",
    "print(cm_test)\n",
    "\n",
    "# Visualize Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Confusion Matrix\n",
    "cm_val = confusion_matrix(y_val.iloc[:, 0], y_val_pred_bilstm)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='Purples', ax=axes[0],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[0].set_title('Bi-LSTM - Confusion Matrix (Validation Set)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Test Confusion Matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='Purples', ax=axes[1],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[1].set_title('Bi-LSTM - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history_bilstm.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history_bilstm.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_title('Bi-LSTM - Training & Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_bilstm.history['loss'], label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history_bilstm.history['val_loss'], label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_title('Bi-LSTM - Training & Validation Loss', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Training   - Accuracy: {train_accuracy:.4f}, Macro F1: {train_macro_f1:.4f}\")\n",
    "print(f\"Validation - Accuracy: {val_accuracy:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "print(f\"Test       - Accuracy: {test_accuracy:.4f}, Macro F1: {test_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1.4 Save Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bi-LSTM model\n",
    "bilstm_model.save('../models/sentiment/bilstm_model.h5')\n",
    "\n",
    "# Save tokenizer\n",
    "with open('../models/sentiment/tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Bi-LSTM model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Bidirectional GRU (Bi-GRU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.1 Imports for Bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU\n",
    "\n",
    "print(\"Bi-GRU libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 Build Bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters (same as Bi-LSTM for fair comparison)\n",
    "NUM_WORDS = 50000        # Vocabulary size\n",
    "MAX_LEN = 128            # Maximum sequence length\n",
    "EMBEDDING_DIM = 300      # Embedding dimension\n",
    "GRU_UNITS = 128          # GRU units\n",
    "BATCH_SIZE = 32          # Training batch size\n",
    "\n",
    "# Note: Using the same tokenized sequences from Bi-LSTM\n",
    "# This ensures fair comparison between LSTM and GRU\n",
    "\n",
    "# Build Bi-GRU model\n",
    "print(\"Building Bi-GRU model...\")\n",
    "print(f\"Configuration: GRU units={GRU_UNITS}, embedding_dim={EMBEDDING_DIM}\")\n",
    "\n",
    "bigru_model = Sequential([\n",
    "    Embedding(input_dim=NUM_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_LEN),\n",
    "    Bidirectional(GRU(GRU_UNITS, return_sequences=False, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "bigru_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Bi-GRU model built successfully!\")\n",
    "bigru_model.summary()\n",
    "\n",
    "# Train Bi-GRU\n",
    "print(\"\\nTraining Bi-GRU model...\")\n",
    "callbacks_gru = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7, verbose=1)\n",
    "]\n",
    "\n",
    "history_bigru = bigru_model.fit(\n",
    "    X_train_pad, y_train.iloc[:, 0],\n",
    "    validation_data=(X_val_pad, y_val.iloc[:, 0]),\n",
    "    epochs=50,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    callbacks=callbacks_gru,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Bi-GRU training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.3 Inference on Bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions...\")\n",
    "y_train_pred_bigru = np.argmax(bigru_model.predict(X_train_pad, verbose=0), axis=1)\n",
    "y_val_pred_bigru = np.argmax(bigru_model.predict(X_val_pad, verbose=0), axis=1)\n",
    "y_test_pred_bigru = np.argmax(bigru_model.predict(X_test_pad, verbose=0), axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"BI-GRU - EVALUATION RESULTS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Training Set Metrics\n",
    "print(\"\\nTRAINING SET:\")\n",
    "print(\"-\"*100)\n",
    "train_accuracy = accuracy_score(y_train.iloc[:, 0], y_train_pred_bigru)\n",
    "train_macro_precision = precision_score(y_train.iloc[:, 0], y_train_pred_bigru, average='macro')\n",
    "train_macro_recall = recall_score(y_train.iloc[:, 0], y_train_pred_bigru, average='macro')\n",
    "train_macro_f1 = f1_score(y_train.iloc[:, 0], y_train_pred_bigru, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {train_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {train_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {train_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {train_macro_f1:.4f}\")\n",
    "\n",
    "# Validation Set Metrics\n",
    "print(\"\\nVALIDATION SET:\")\n",
    "print(\"-\"*100)\n",
    "val_accuracy = accuracy_score(y_val.iloc[:, 0], y_val_pred_bigru)\n",
    "val_macro_precision = precision_score(y_val.iloc[:, 0], y_val_pred_bigru, average='macro')\n",
    "val_macro_recall = recall_score(y_val.iloc[:, 0], y_val_pred_bigru, average='macro')\n",
    "val_macro_f1 = f1_score(y_val.iloc[:, 0], y_val_pred_bigru, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {val_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {val_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {val_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {val_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val.iloc[:, 0], y_val_pred_bigru, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Test Set Metrics\n",
    "print(\"\\nTEST SET:\")\n",
    "print(\"-\"*100)\n",
    "test_accuracy = accuracy_score(y_test.iloc[:, 0], y_test_pred_bigru)\n",
    "test_macro_precision = precision_score(y_test.iloc[:, 0], y_test_pred_bigru, average='macro')\n",
    "test_macro_recall = recall_score(y_test.iloc[:, 0], y_test_pred_bigru, average='macro')\n",
    "test_macro_f1 = f1_score(y_test.iloc[:, 0], y_test_pred_bigru, average='macro')\n",
    "\n",
    "print(f\"Accuracy:         {test_accuracy:.4f}\")\n",
    "print(f\"Macro Precision:  {test_macro_precision:.4f}\")\n",
    "print(f\"Macro Recall:     {test_macro_recall:.4f}\")\n",
    "print(f\"Macro F1-Score:   {test_macro_f1:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test.iloc[:, 0], y_test_pred_bigru, \n",
    "                          target_names=['Neutral', 'Negative', 'Positive'],\n",
    "                          digits=4))\n",
    "\n",
    "# Confusion Matrices\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm_test = confusion_matrix(y_test.iloc[:, 0], y_test_pred_bigru)\n",
    "print(cm_test)\n",
    "\n",
    "# Visualize Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Validation Confusion Matrix\n",
    "cm_val = confusion_matrix(y_val.iloc[:, 0], y_val_pred_bigru)\n",
    "sns.heatmap(cm_val, annot=True, fmt='d', cmap='YlOrBr', ax=axes[0],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[0].set_title('Bi-GRU - Confusion Matrix (Validation Set)', fontweight='bold', fontsize=14)\n",
    "axes[0].set_ylabel('True Label', fontsize=12)\n",
    "axes[0].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "# Test Confusion Matrix\n",
    "sns.heatmap(cm_test, annot=True, fmt='d', cmap='YlOrBr', ax=axes[1],\n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "axes[1].set_title('Bi-GRU - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "axes[1].set_ylabel('True Label', fontsize=12)\n",
    "axes[1].set_xlabel('Predicted Label', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history_bigru.history['accuracy'], label='Train Accuracy', linewidth=2, marker='o', markersize=4)\n",
    "axes[0].plot(history_bigru.history['val_accuracy'], label='Validation Accuracy', linewidth=2, marker='s', markersize=4)\n",
    "axes[0].set_title('Bi-GRU - Training & Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_bigru.history['loss'], label='Train Loss', linewidth=2, marker='o', markersize=4)\n",
    "axes[1].plot(history_bigru.history['val_loss'], label='Validation Loss', linewidth=2, marker='s', markersize=4)\n",
    "axes[1].set_title('Bi-GRU - Training & Validation Loss', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*100)\n",
    "print(f\"Training   - Accuracy: {train_accuracy:.4f}, Macro F1: {train_macro_f1:.4f}\")\n",
    "print(f\"Validation - Accuracy: {val_accuracy:.4f}, Macro F1: {val_macro_f1:.4f}\")\n",
    "print(f\"Test       - Accuracy: {test_accuracy:.4f}, Macro F1: {test_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.4 Save Bi-GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Bi-GRU model\n",
    "bigru_model.save('../models/sentiment/bigru_model.h5')\n",
    "\n",
    "print(\"Bi-GRU model saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Imports for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from transformers import DataCollatorWithPadding\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"BERT libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Build BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load multilingual BERT tokenizer and model\n",
    "MODEL_NAME = 'bert-base-multilingual-cased'\n",
    "MAX_LENGTH = 128\n",
    "\n",
    "print(f\"Loading {MODEL_NAME} tokenizer...\")\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Tokenize data\n",
    "print(\"Tokenizing data for BERT...\")\n",
    "train_encodings = bert_tokenizer(\n",
    "    X_train['Text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "val_encodings = bert_tokenizer(\n",
    "    X_val['Text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "test_encodings = bert_tokenizer(\n",
    "    X_test['Text'].tolist(),\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_tensors='tf'\n",
    ")\n",
    "\n",
    "print(\"Tokenization completed!\")\n",
    "\n",
    "# Create TensorFlow datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    y_train.values.ravel()\n",
    ")).batch(16)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    y_val.values.ravel()\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_encodings),\n",
    "    y_test.values.ravel()\n",
    ")).batch(16)\n",
    "\n",
    "# Load BERT model for sequence classification\n",
    "print(f\"\\nLoading {MODEL_NAME} model...\")\n",
    "bert_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3\n",
    ")\n",
    "\n",
    "# Compile model\n",
    "bert_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"BERT model loaded and compiled successfully!\")\n",
    "\n",
    "# Train BERT\n",
    "print(\"\\nTraining BERT model...\")\n",
    "history_bert = bert_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"BERT training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Inference on BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "print(\"Making predictions with BERT...\")\n",
    "y_train_pred_bert = np.argmax(bert_model.predict(train_dataset).logits, axis=1)\n",
    "y_val_pred_bert = np.argmax(bert_model.predict(val_dataset).logits, axis=1)\n",
    "y_test_pred_bert = np.argmax(bert_model.predict(test_dataset).logits, axis=1)\n",
    "\n",
    "# Evaluation\n",
    "print(\"=\"*80)\n",
    "print(\"BERT - EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for dataset_name, y_true, y_pred in [('Training', y_train, y_train_pred_bert), \n",
    "                                       ('Validation', y_val, y_val_pred_bert),\n",
    "                                       ('Test', y_test, y_test_pred_bert)]:\n",
    "    print(f\"\\n{dataset_name} Set:\")\n",
    "    print(\"-\"*80)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    macro_f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Macro F1-Score: {macro_f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['Neutral', 'Negative', 'Positive']))\n",
    "\n",
    "# Confusion Matrix for Test Set\n",
    "print(\"\\nConfusion Matrix (Test Set):\")\n",
    "cm = confusion_matrix(y_test, y_test_pred_bert)\n",
    "print(cm)\n",
    "\n",
    "# Visualize Confusion Matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', \n",
    "            xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "            yticklabels=['Neutral', 'Negative', 'Positive'])\n",
    "plt.title('BERT - Confusion Matrix (Test Set)', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(history_bert.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "axes[0].plot(history_bert.history['val_accuracy'], label='Validation Accuracy', linewidth=2)\n",
    "axes[0].set_title('BERT - Training & Validation Accuracy', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Epoch', fontsize=11)\n",
    "axes[0].set_ylabel('Accuracy', fontsize=11)\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(history_bert.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[1].plot(history_bert.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "axes[1].set_title('BERT - Training & Validation Loss', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Epoch', fontsize=11)\n",
    "axes[1].set_ylabel('Loss', fontsize=11)\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Save BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save BERT model\n",
    "bert_model.save_pretrained('../models/sentiment/bert_model')\n",
    "bert_tokenizer.save_pretrained('../models/sentiment/bert_model')\n",
    "\n",
    "print(\"BERT model and tokenizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comparison of All Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Collect All Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all test predictions\n",
    "models_predictions = {\n",
    "    'SVM': y_test_pred_svm,\n",
    "    'MNB': y_test_pred_mnb,\n",
    "    'RF': y_test_pred_rf,\n",
    "    'Bi-LSTM': y_test_pred_bilstm,\n",
    "    'Bi-GRU': y_test_pred_bigru,\n",
    "    'BERT': y_test_pred_bert\n",
    "}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "results = []\n",
    "class_names = ['Neutral', 'Negative', 'Positive']\n",
    "\n",
    "for model_name, y_pred in models_predictions.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    precision_per_class = precision_score(y_test, y_pred, average=None)\n",
    "    recall_per_class = recall_score(y_test, y_pred, average=None)\n",
    "    f1_per_class = f1_score(y_test, y_pred, average=None)\n",
    "    \n",
    "    results.append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Macro F1': macro_f1,\n",
    "        'Neutral Precision': precision_per_class[0],\n",
    "        'Neutral Recall': recall_per_class[0],\n",
    "        'Neutral F1': f1_per_class[0],\n",
    "        'Negative Precision': precision_per_class[1],\n",
    "        'Negative Recall': recall_per_class[1],\n",
    "        'Negative F1': f1_per_class[1],\n",
    "        'Positive Precision': precision_per_class[2],\n",
    "        'Positive Recall': recall_per_class[2],\n",
    "        'Positive F1': f1_per_class[2]\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"Metrics collection completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Overall Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*100)\n",
    "print(\"MODEL COMPARISON - OVERALL METRICS (TEST SET)\")\n",
    "print(\"=\"*100)\n",
    "print(results_df[['Model', 'Accuracy', 'Macro F1']].to_string(index=False))\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Visualize overall metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "models = results_df['Model'].values\n",
    "x_pos = np.arange(len(models))\n",
    "\n",
    "# Accuracy comparison\n",
    "axes[0].bar(x_pos, results_df['Accuracy'], color='steelblue', edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Model Comparison - Accuracy', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xticks(x_pos)\n",
    "axes[0].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[0].set_ylim([0, 1])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['Accuracy']):\n",
    "    axes[0].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# Macro F1 comparison\n",
    "axes[1].bar(x_pos, results_df['Macro F1'], color='coral', edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_xlabel('Model', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Macro F1-Score', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Model Comparison - Macro F1-Score', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xticks(x_pos)\n",
    "axes[1].set_xticklabels(models, rotation=45, ha='right')\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(results_df['Macro F1']):\n",
    "    axes[1].text(i, v + 0.02, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Class-wise Precision, Recall, F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"CLASS-WISE METRICS (TEST SET)\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Display class-wise metrics\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    print(f\"\\n{class_name.upper()} Class:\")\n",
    "    print(\"-\"*100)\n",
    "    class_metrics = results_df[['Model', f'{class_name} Precision', f'{class_name} Recall', f'{class_name} F1']]\n",
    "    print(class_metrics.to_string(index=False))\n",
    "\n",
    "# Visualize class-wise F1-scores\n",
    "fig, axes = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "for idx, class_name in enumerate(class_names):\n",
    "    precision = results_df[f'{class_name} Precision'].values\n",
    "    recall = results_df[f'{class_name} Recall'].values\n",
    "    f1 = results_df[f'{class_name} F1'].values\n",
    "    \n",
    "    x = np.arange(len(models))\n",
    "    width = 0.25\n",
    "    \n",
    "    axes[idx].bar(x - width, precision, width, label='Precision', color='#5DADE2')\n",
    "    axes[idx].bar(x, recall, width, label='Recall', color='#E74C3C')\n",
    "    axes[idx].bar(x + width, f1, width, label='F1-Score', color='#52BE80')\n",
    "    \n",
    "    axes[idx].set_xlabel('Model', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Score', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{class_name} Class Metrics', fontsize=13, fontweight='bold')\n",
    "    axes[idx].set_xticks(x)\n",
    "    axes[idx].set_xticklabels(models, rotation=45, ha='right', fontsize=9)\n",
    "    axes[idx].set_ylim([0, 1])\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Confusion Matrices Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create confusion matrices for all models\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "cmaps = ['Blues', 'Greens', 'Oranges', 'Purples', 'YlOrBr', 'Reds']\n",
    "\n",
    "for idx, (model_name, y_pred) in enumerate(models_predictions.items()):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=cmaps[idx], ax=axes[idx],\n",
    "                xticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "                yticklabels=['Neutral', 'Negative', 'Positive'],\n",
    "                cbar_kws={'label': 'Count'})\n",
    "    \n",
    "    axes[idx].set_title(f'{model_name} - Confusion Matrix', fontweight='bold', fontsize=13)\n",
    "    axes[idx].set_ylabel('True Label', fontsize=11)\n",
    "    axes[idx].set_xlabel('Predicted Label', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Save Comparison Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it doesn't exist\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "# Save results to CSV\n",
    "results_df.to_csv('../results/sentiment_model_comparison.csv', index=False)\n",
    "print(\"Results saved to ../results/sentiment_model_comparison.csv\")\n",
    "\n",
    "# Save detailed report\n",
    "with open('../results/sentiment_detailed_report.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(\"SENTIMENT ANALYSIS - MODEL COMPARISON REPORT\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall Performance:\\n\")\n",
    "    f.write(\"-\"*100 + \"\\n\")\n",
    "    f.write(results_df[['Model', 'Accuracy', 'Macro F1']].to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Class-wise Detailed Metrics:\\n\")\n",
    "    f.write(\"=\"*100 + \"\\n\")\n",
    "    f.write(results_df.to_string(index=False))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    # Classification reports for each model\n",
    "    for model_name, y_pred in models_predictions.items():\n",
    "        f.write(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "        f.write(f\"{model_name} - DETAILED CLASSIFICATION REPORT\\n\")\n",
    "        f.write(\"=\"*100 + \"\\n\")\n",
    "        f.write(classification_report(y_test, y_pred, target_names=['Neutral', 'Negative', 'Positive']))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(\"Detailed report saved to ../results/sentiment_detailed_report.txt\")\n",
    "print(\"\\nAll models trained, evaluated, and saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
